name: Deploy to AWS Lambda

on:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  build-and-deploy:
    name: Build package and deploy to Lambda
    runs-on: ubuntu-latest
    env:
      AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Upgrade pip
        run: python -m pip install --upgrade pip

      - name: Install dependencies into package dir
        run: |
          set -e
          rm -rf package || true
          mkdir package
          # Exclude heavy libraries that are already available in Lambda or can be reduced
          # pandas and pyarrow are very large. We should try to use a Lambda Layer or exclude them if possible.
          # For now, let's try to install without dependencies for boto3 (it's in Lambda runtime)
          # and use --no-deps for some if needed, but pip install -t installs everything.
          
          # Strategy: Install requirements but remove heavy stuff that might not be needed or use layers.
          # However, pandas IS needed. The issue is the size limit (50MB zipped direct upload).
          
          pip install --upgrade -r requirements.txt --target package
          
          # Cleanup to reduce size
          cd package
          find . -type d -name "tests" -exec rm -rf {} +
          find . -type d -name "__pycache__" -exec rm -rf {} +
          rm -rf *.dist-info *.egg-info
          
          # Boto3 and botocore are included in the AWS Lambda Python runtime.
          # Removing them saves significant space (~50MB+ unzipped).
          rm -rf boto3* botocore* s3transfer*
          cd ..

      - name: Copy project files into package
        run: |
          # Copy your script(s) so that ingest_b3.py is at the root of the zip
          cp -r scripts/* package/ || true
          # If you have other modules or folders that the function needs, copy them too

      - name: Zip package contents
        run: |
          cd package
          # Zip the CONTENTS of package so files are at the zip root (important for Lambda handler)
          zip -r ../function.zip . -x "*.pyc" "__pycache__/*"
          cd ..
          ls -lh function.zip

      - name: Debug Auth Configuration
        run: |
          echo "Checking authentication configuration..."
          if [ -z "$AWS_ROLE_TO_ASSUME" ]; then
            echo "AWS_ROLE_TO_ASSUME is empty. Workflow will attempt to use Access Keys."
          else
            echo "AWS_ROLE_TO_ASSUME is set. Workflow will attempt to use OIDC."
          fi
          
          # Check if region is set (without printing secret if it was one, though region usually isn't)
          if [ -z "${{ secrets.AWS_REGION }}" ]; then
             echo "WARNING: AWS_REGION secret is empty!"
          else
             echo "AWS_REGION is set."
          fi

      - name: Configure AWS credentials (assume role via OIDC)
        if: env.AWS_ROLE_TO_ASSUME != ''
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Configure AWS credentials (access key fallback)
        if: env.AWS_ROLE_TO_ASSUME == ''
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Verify AWS identity
        run: aws sts get-caller-identity

      - name: Update Lambda function code
        env:
          LAMBDA_NAME: ${{ secrets.LAMBDA_FUNCTION_NAME }}
          # Optional secrets used for S3 fallback when the package is too large to upload directly
          DEPLOY_S3_BUCKET: ${{ secrets.DEPLOY_S3_BUCKET }}
          DEPLOY_S3_KEY_PREFIX: ${{ secrets.DEPLOY_S3_KEY_PREFIX }}
        run: |
          if [ -z "${LAMBDA_NAME}" ]; then
            echo "ERROR: Set the secret LAMBDA_FUNCTION_NAME"
            exit 1
          fi

          ZIP_FILE=function.zip
          if [ ! -f "${ZIP_FILE}" ]; then
            echo "ERROR: ${ZIP_FILE} not found"
            exit 1
          fi

          # Get file size in bytes (portable between linux stat implementations)
          filesize=$(stat -c%s "${ZIP_FILE}" 2>/dev/null || stat -f%z "${ZIP_FILE}" 2>/dev/null || echo 0)
          echo "Package size: ${filesize} bytes"

          # Threshold (bytes): use ~70MB to stay under the UpdateFunctionCode direct upload limit.
          threshold=70000000

          if [ "${filesize}" -lt "${threshold}" ]; then
            echo "Uploading directly via aws lambda update-function-code (size below threshold)"
            aws lambda update-function-code --function-name "${LAMBDA_NAME}" --zip-file fileb://"${ZIP_FILE}"
          else
            echo "Package is larger than ${threshold} bytes. Using S3 upload + UpdateFunctionCode from S3."
            if [ -z "${DEPLOY_S3_BUCKET}" ]; then
              echo "ERROR: DEPLOY_S3_BUCKET secret is not set. To enable large-package deployments set the secret DEPLOY_S3_BUCKET to an S3 bucket name."
              echo "You can set DEPLOY_S3_KEY_PREFIX (optional) to control the uploaded key prefix."
              exit 1
            fi

            key_prefix=${DEPLOY_S3_KEY_PREFIX:-"deploy"}
            timestamp=$(date +%Y%m%d-%H%M%S)
            s3_key="${key_prefix}/${LAMBDA_NAME}-${timestamp}.zip"

            echo "Uploading ${ZIP_FILE} to s3://${DEPLOY_S3_BUCKET}/${s3_key}"
            aws s3 cp "${ZIP_FILE}" "s3://${DEPLOY_S3_BUCKET}/${s3_key}"

            echo "Updating Lambda from S3 object"
            aws lambda update-function-code --function-name "${LAMBDA_NAME}" --s3-bucket "${DEPLOY_S3_BUCKET}" --s3-key "${s3_key}"
          fi

      - name: Confirm deployment
        env:
          LAMBDA_NAME: ${{ secrets.LAMBDA_FUNCTION_NAME }}
        run: |
          aws lambda get-function --function-name "${LAMBDA_NAME}" --query 'Configuration.[FunctionName,LastModified,Runtime]' --output table

    # Notes:
    # - To use the OIDC flow, create an IAM Role with a trust relationship that allows
    #   GitHub's OIDC provider to assume it. Store the role ARN in the secret AWS_ROLE_TO_ASSUME.
    # - If you prefer static keys, set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in repo secrets.
    # - For improved security, prefer the OIDC flow and grant only the specific S3/Lambda permissions needed.
